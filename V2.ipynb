{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1-اول خطوة ان استدعي  المكتبات\n"
      ],
      "metadata": {
        "id": "L9mXwLmx0VY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# علي ان استدعي المكتبات التي احتاجها و التي تكون مناسبة لنموذجي\n",
        "import numpy as np # و هي للاعداد و المصفوفات و اي عمليات حسابيةnumpyمكتبة\n",
        "import pandas as pd #(الجداول)لتنظيف و تحليل و معالجة البيانات pandas مكتبة\n",
        "import matplotlib.pyplot as plt #و هي التي تساعدني على الرسم\n",
        "#و هنا هذه المكتبات التي احتاجها في النموذج و ليس فقط الاستكشاف و التنظيف\n",
        "# و اولها هنا هذه المكتبة التي نقوم من خلالها بتقسيم البيانات\n",
        "from sklearn.model_selection import train_test_split\n",
        "#و هذه الخوارزمية الخاصة للانحدار اللوجستي\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#و هذه لكي نقيس دقة النموذج\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "#و هذه المكتبة التي احتجتها لاضافة التحسينات و التي هي ان استخدم خوارزمية\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "owgrwJqB0XbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- الخاص بي و استدعي ملف البيناتdriveوبعدها سوف اتصل بال"
      ],
      "metadata": {
        "id": "qx8PaKzW1Id7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byoG6Jey1G4F",
        "outputId": "bbf16fd5-962c-470c-a9e8-38c3841d6c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#لقراءة ملف البيانات الخاص بي\n",
        "df=pd.read_csv('/content/drive/MyDrive/AI/healthcare-dataset-stroke-data.csv')"
      ],
      "metadata": {
        "id": "O80YqDK6z_Fi",
        "outputId": "2730f673-923a-4462-90b9-022ad09099ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/AI/healthcare-dataset-stroke-data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1376991157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#لقراءة ملف البيانات الخاص بي\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/AI/healthcare-dataset-stroke-data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/AI/healthcare-dataset-stroke-data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-و الان سوف أبدأ بان استكشف بياناتي و أفهمها\n"
      ],
      "metadata": {
        "id": "nN4xy1LD1ERc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#وهنا اعرف ما هي حجم بياناتي فمن خلال اشوف عدد الاعمدة و الصفوف\n",
        "print(\"Dataset shape:\")\n",
        "df.shape\n",
        "#و ايضا فيمكن استخدام كل من :\n",
        "#df.shape[0]   #و هي لعرض عدد الصفوف فقط\n",
        "#df.shape[1]   #لعرض عدد الاعمدة فقط"
      ],
      "metadata": {
        "id": "_lfQKoz_z_LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و في هذه الخطوة نعرف ما هي اسماء الاعمدة و يصبح لنا تصور لطبيعة البيانات و يمكن فهمها اكثر\n",
        "df.columns"
      ],
      "metadata": {
        "id": "Q9iu2zNd0hZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و هنا لنعرض ما اهي انواع البيانات في كل عامود\n",
        "df.dtypes\n"
      ],
      "metadata": {
        "id": "a6ggKvPD0j2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#فهي تعطيني اسماء الاعمدة و نوع البيانات الموجوده بها و ايضا عدد البيانات في كل عامود اي اعرف اذا كان هناك نقص في  البياناتinfoولكن فعندما استخدم\n",
        "#لكن بالطبع هم مهمين لانرتب افكارنا بشكل مبدأي و بعدها نتعمق بالبيانات بشكل أكبر  ، columns وdtypes اي فانها تعمل عمل كل من\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ECrQvKIc0l53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (5)فهو يظهر اول الصفوفheadو من خلال اسخدام ال\n",
        "print(\" first 5 rows :\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "gHqo2bQI0nve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(5)باخد بيانات من اخر الصفوف tailمن خلال\n",
        "print(\"Last 5 rows:\")\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "aaRFO7Aj0pyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #و هو لعمل وصف احصائي للبيانات\n",
        " #نعرف عدد القيم الموجودة اي بدون اي فرغات  countو هنا أولا من خلال\n",
        " # نعرف عدد القيم المختلفة ، و هنا فيما يخص النصوص uniqueو من خلال\n",
        " #فهي عدد مرات تكرارهاfreqو هي أكثر قيمة متكررة أما   top\n",
        " #المتوسط الحسابي اي متوسط الشيء فمثلا العمر 43.22 فهذا المتوسط فيوجد اكثر و أقل فتكون المعادلة مجموع القيم تقسيم عدد القيم  mean\n",
        " # Outliersالانحراف المعياري اي كمية قديش البيانات متشتته و بالتالي نعرف اذا كان هناك std\n",
        " #فيجب ان يكون مقدار التشتت قريب من الوسيط الحسابي meanبال stdو ذلك من خلال ان نقارن ال\n",
        " #و هي أصغير قيمة min\n",
        " #أكبر قيمة max\n",
        " #Outliersو ايضا فمن خلالهم و القيم % نعرف اذا كان هناك\n",
        "\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "l_HYu__u0sE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#لمعرفة كم قيمه مختلفة في كل عامود\n",
        "#و بالتالني نعرف اذا كان هناك قيم لا نحتاجها في عامود محدد\n",
        "#و قد استخدت من خلال الوصف الاحصائي سابقا و لكن قد عرضتها منفصلة\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "_ON-3qZL0uEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و هنا لاعرف عدد الصفوف المتكررة و التي سوف يعبر عنها برقم\n",
        "#و هنا لكل سطر يكون يوجد كتركيبة و في النهاية يوجد عدد تكرار السطر\n",
        "df.value_counts()\n",
        "#و حتى فهناك طريقة اخر لمعرف مجمل العدد للتكرار باكمله من خلال\n",
        "#و هو لمعرفة عدد القيم المتكررة او الصفوف المتكرة\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "JoasJqF70wKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#لمعرفة عدد القيم الفارغة\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "jPGc7D6E0zDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4-و الان بعد ان استكشفت البيانات بعمق فعلي تنظيفها\n"
      ],
      "metadata": {
        "id": "kmS8fsI307vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#لانه لا يؤثر على النموذج و لا نحتاجهidو أول خطوة سوف أبدأ بها و هي ان احذف عمود\n",
        "#و قد تأكدة من خلالنه انه لا يوجد اي تكرارات\n",
        "df = df.drop('id', axis=1)"
      ],
      "metadata": {
        "id": "K4CGUA0V1Ws5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و العمود الذي يليه و هو العمر و بما أن فيه 3 قيم اريد أن اعرف القيمة الثاله و هي أخرى ما عددها لان اذا كثيره سوف أعوضها اما اذا قليلة ممكن حذفها\n",
        "df['gender'].value_counts()\n",
        "#و بما أن قيمة أخرى فقط واحدة من نسبة البيانات و هي تقريبا 5000 اذا فيمكنني حذها دون تأثير على البيانات\n",
        "df = df[df['gender'] != 'Other']\n",
        "#df['gender'].value_counts()#للتأكد\n"
      ],
      "metadata": {
        "id": "uN0jQpBo1YbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# و بالنسبة الى عمود العمر فلاحظت من خلال التحليل و معرفة اقل قيمة ان هناك قيم أقل من سنة أي أشهر اذا هاد رقم مش منطقي لمرض سكتات دماغية\n",
        "#و أولا يجس ان اعرف ما هي نسبتهم لكي أعرف اعوضهم بقيمة أخرى أو أحذفهم من خلال :\n",
        "df[df['age'] < 1].shape[0]\n",
        "#و بما أن الرقم صغير جدا بالنسبة لكمية البيانات فهوأقل من 1% و لهذا فقررت أن احذفهم\n",
        "#لان حتى تعويضهم مثلا لسنه واحده ممكن ان سبب قيم شاذه\n",
        "df = df[df['age'] >= 1]\n",
        "#df['age'].min()#للتأكد"
      ],
      "metadata": {
        "id": "GPBhh7Jz1aZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و العمود الذي يليه و هو طبيعه العمل و أولا سوف أرى الفئات لان هناك 5 فئات\n",
        "df['work_type'].value_counts()\n",
        "#فأجد ان فئة لم يعمل قليله جدا و هي مصنفة كفئة لوحدها و لهذا قررت أن أدمجها مع فئة الأطفال لانهم بالطبع لم يعملو و بهذا لن يتأثر النموذج بل هذا أفضل\n",
        "# دمج Never_worked مع children\n",
        "df['work_type'] = df['work_type'].replace({'Never_worked': 'children'})\n",
        "#df['work_type'].value_counts()#للتأكد"
      ],
      "metadata": {
        "id": "0xUJK3DC1cJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 106meanو بالنسبة لعمود مستوى الجلوكوز فعندما عملت سابقا وصف اصائي في استكشاف البيانات تبين لي ان هناك قيم شاذه فقد كان ال\n",
        "#45 stdو كان ال\n",
        "#و لكن فكانت أعلى قيمه 271\n",
        "#و لهذا فسوف اعوض اي قيم اكبر من 169 بالوسيط و بهذا احافظ على البيانات دون حذفها و يحسن من النموذج\n",
        "#و أولا نحسب الوسيط\n",
        "median_glucose = df['avg_glucose_level'].median()\n",
        "\n",
        "# و هنا نستبدل القيم الشاذة بالوسيط\n",
        "#لانها للقيم الشاذهapplyو استخدمت\n",
        "#169و اما الرقم فعندما أجمع الوسط الحسابي بضرب الانحراف مرتين فهو يساوي\n",
        "df['avg_glucose_level'] = df['avg_glucose_level'].apply(\n",
        "    lambda x: median_glucose if x > 169.3 else x\n",
        ")\n",
        "\n",
        "#df['avg_glucose_level'].describe()#للتأكد\n",
        "\n"
      ],
      "metadata": {
        "id": "3vsejntj1d2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و بالنسبة لعمود كتلة الجسم فقد كان به قيم فارغه و لهذا فقررت أن أعوضها بالمتوسط و بهذا فتكون مناسبة\n",
        "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())\n",
        "df['bmi'] = df['bmi'].round(2)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fOkKss4z1fwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و بالنسبة لعمود التدخين و الذي فيه 4 قيم و بينها انه غير معروف و لهذا فعلي ان اعوضها بقيمة محددة\n",
        "df['smoking_status'].value_counts()\n",
        "#و بالنظر الي كميتها فهي كبيرة و بالتاكيد لا يمكن حذفها و لهذا سوف اعوضها باكثر قيمة تكرارا و اجد ان هذا منطقي أكثر\n",
        "#و لهذا علي ان اعرف اي قيمة أكثر تكرار أولا من خلا\n",
        "mode_smoke = df['smoking_status'].mode()[0]\n",
        "# و بعدها نستبدل القيمة بالmode\n",
        "df['smoking_status'] = df['smoking_status'].replace('Unknown', mode_smoke)\n",
        "\n",
        "# التأكد من النتائج\n",
        "df['smoking_status'].value_counts()"
      ],
      "metadata": {
        "id": "pS67K85t1hs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5-و بما أن نظامي هو الانحدار اللوجستي أي ان القيم يجب ان تكون رقمية و لهذا فسوف احول الأعمدة لدي الى قيم رقمية ليفهمها\n"
      ],
      "metadata": {
        "id": "lNwHz1V11mAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# و الأن سوف احول الاعمدة الى ارقام بالكامل لكي تفهمها الخوارزمية\n",
        "#  الى رقم بما انه ذكر و انثى gender أولا ان احول عمود\n",
        "df['gender'] = df['gender'].map({'Female':0, 'Male':1})\n",
        "# الى رقم بما انه نعم و لا ever_marriedو ايضا تحويل\n",
        "df['ever_married'] = df['ever_married'].map({'No':0, 'Yes':1})\n",
        "# بما ناه ريفي و حضري فقط  Residence_typeو ايضا تحويل\n",
        "df['Residence_type'] = df['Residence_type'].map({'Rural':0, 'Urban':1})\n"
      ],
      "metadata": {
        "id": "BRwolBn71m95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#و بالنسبة الى عمود العمل و التدخين و التي بها العديد من القيم النصية\n",
        "#فنحول القيم لارقام و نحدد ان نريد هذه الاعمدة بين اولا فمن خلال\n",
        "#و حذفت اول فئة لكل عمود لتجنب التكرار\n",
        "#فهو ياخذ كل نص في العمود و يصنع له عمود جديد باسمه و بهذا فمثلا عمود العمل يصبح لكل فئة عمود لوحدها لاستطيع ان احوله على 0 و 1get_dummiesو من خلال\n",
        "df = pd.get_dummies(df, columns=['work_type','smoking_status'], drop_first=True)\n",
        "#و لهذا فاحتاج الى ان احولخها ل1 و 0من خلالtrue , falseو الان تم تقسيم الاعمدة و تحولت الى\n",
        "bool_cols = df.select_dtypes(include='bool').columns  # يلتقط كل الأعمدة True/False\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2nv_eSS11ra5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6-و بعد ان اصبحت بياناتي جاخهزة يمكنني البدء ببناء النموذج\n"
      ],
      "metadata": {
        "id": "98KwZ7Uw1wjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x,yو أول خطوة اقوم بها هي ان احدد ال\n",
        "#و هنا احدد المتغير الهدف الذي اريد ان اتنبأ به و هو حدوث السكتة الدماغية\n",
        "y = df['stroke']\n",
        "#و هنا احدد المتغيرات المستقلة و هي جميع الاعمدة ما عدا الهدف\n",
        "x = df.drop('stroke', axis=1)\n",
        "#للتأكد\n",
        "print(\"Shape of x:\")\n",
        "print(x.shape)\n",
        "print(\"Shape of y:\")\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "qHlRRd9S1z7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و هنا اقوم بتقسيم البيانات الى بيانات تدريب و اختبار\n",
        "#و استخدمت stratify و لكي احافظ على نفس نسبة الفئات 0 و 1\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=33,\n",
        "# لان البيانات غير متوازنة فان فئة الغير مصابه اكثر بكثير من الصابة stratifyهنا أضفت\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "#للتأكد من حجم البيانات بعد التقسيم\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "id": "S6fgSmHp11rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و هنا اقوم بإنشاء نموذج الانحدار اللوجستي\n",
        "# class_weight='balanced'ولكن لان الفئات لدي غير متوازنه فان نسبة المصابين قليله جدا و الغير مصابين كثير فاذا لم نعالجه سوف يحسب ان جميعهم غير مصابين و لهذا فقد استخدمت\n",
        "model = LogisticRegression(max_iter=999, class_weight='balanced')\n",
        "\n",
        "#تدريب النموذج\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "A9aOcW3z13gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#و هنا اقوم باستخدام النموذج المدرب لعمل تنبؤات على بيانات الاختبار\n",
        "y_pred = model.predict(x_test)\n",
        "#و هنا اقوم بحساب دقة النموذج من خلال مقارنة القيم الحقيقية مع القيم المتوقعة\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the model:\")\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "Nw7kzZcJ15Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#و هنا احسب مصفوفة الالتباس لمعرفة عدد الحالات الصحيحة و الخاطئة لكل فئة\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "#و هنا اعرض تقرير التصنيف الكامل الذي يحتوي على precision و recall و f1-score\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6PYrzmyX17JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "التعديل 2"
      ],
      "metadata": {
        "id": "pjtb2-YVtERp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decision Tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# هنا أنشئ نموذج شجرة القرار\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    random_state=42,\n",
        "    class_weight='balanced'   # أضفناه لأن بياناتنا غير متوازنة\n",
        ")\n",
        "\n",
        "# و الان تدريب النموذج على نفس بيانات التدريب\n",
        "dt_model.fit(x_train, y_train)\n",
        "\n",
        "# التنبؤ على بيانات الاختبار\n",
        "y_pred_tree = dt_model.predict(x_test)\n",
        "\n",
        "# و هتنا نحسب الدقة مثل سابقالنقيس الدقة\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "print(\"Decision Tree Accuracy:\")\n",
        "print(accuracy_tree)\n",
        "\n",
        "# مصفوفة الالتباس\n",
        "cm_tree = confusion_matrix(y_test, y_pred_tree)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_tree)\n",
        "\n",
        "# تقرير التصنيف\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_tree))"
      ],
      "metadata": {
        "id": "wrdXWFxttJ3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "# حفظ الملف بصيغة PKL\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# تنزيل الملف على جهازك\n",
        "files.download('model.pkl')\n"
      ],
      "metadata": {
        "id": "TUzQMKaYX8vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyflakes\n",
        "!pyflakes stroke_model.py"
      ],
      "metadata": {
        "id": "9mmMfdOYZIqw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}